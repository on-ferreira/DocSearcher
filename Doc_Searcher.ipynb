{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lwMgzD0JvHI"
   },
   "source": [
    "---\n",
    "\n",
    "## Instalando Dependências\n",
    "\n",
    "Antes de começar a utilizar o DocSearcher, é necessário instalar suas dependências. Você pode instalar todas as bibliotecas necessárias executando o seguinte comando:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Certifique-se de que está na pasta raiz do projeto, onde está localizado o artquivo `requirements.txt`.\n",
    "\n",
    "Este arquivo contém todas as dependências necessárias para executar o DocSearcher, incluindo:\n",
    "\n",
    "- `google-generativeai`\n",
    "- `docx2python`\n",
    "- `numpy`\n",
    "- `pandas`\n",
    "- `scikit-learn`\n",
    "\n",
    "Além disso, para utilizar as funcionalidades de pesquisa do Google, é necessário configurar uma chave de API do Google. Certifique-se de ter uma chave de API válida e substitua `GOOGLE_API_KEY` no código pelo valor correspondente à sua chave de API.\n",
    "\n",
    "Após a instalação bem-sucedida das dependências e a configuração da chave de API do Google, você estará pronto para utilizar todas as funcionalidades oferecidas pelo DocSearcher.\n",
    "\n",
    "Além disso, para a execução adequada do DocSearcher, foi escolhido o modelo de incorporação \"models/embedding-001\" para processar os dados e fornecer resultados das buscas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hF46sMG7ld7Q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "from docx2python import docx2python\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "GOOGLE_API_KEY=\"YOUR API GOES HERE\"\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "model = \"models/embedding-001\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vv6ssrxnM7Wx"
   },
   "source": [
    "Para utilizar a função abaixo e extrair o conteúdo de documentos .docx de um diretório específico, basta fornecer o caminho para esse diretório como argumento da função."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xy8XyShdsrRX"
   },
   "outputs": [],
   "source": [
    "def extract_docx_contents(docs_path):\n",
    "    \"\"\"\n",
    "    Extracts the content of .docx documents from a specific directory.\n",
    "\n",
    "    Args:\n",
    "        docs_path (str): The path to the directory containing the .docx documents.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, where each dictionary contains the title and content of a document.\n",
    "\n",
    "    Example:\n",
    "        >>> docs_path = \"/path/to/your/docs\"\n",
    "        >>> documents = extract_docx_contents(docs_path)\n",
    "    \"\"\"\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for doc in os.listdir(docs_path):\n",
    "        if doc.endswith(\".docx\"):\n",
    "            path_file = os.path.join(docs_path, doc)\n",
    "            doc_title = os.path.splitext(doc)[0]\n",
    "            docx_content = docx2python(path_file)\n",
    "           \n",
    "            if docx_content.body and len(docx_content.body) > 1:\n",
    "                text = ''.join([''.join(subsublist) for sublist in docx_content.body[1] for subsublist in sublist])\n",
    "            elif docx_content.body:\n",
    "                text = docx_content.text\n",
    "\n",
    "            document = {\n",
    "                    \"Title\": doc_title,\n",
    "                    \"Content\": text\n",
    "                }\n",
    "            documents.append(document)\n",
    "            \n",
    "\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDo-BBkyNJxw"
   },
   "source": [
    "O seguinte bloco de código realiza a pré-processamento dos dados extraídos dos documentos .docx. Isso inclui a formatação dos títulos e conteúdos dos documentos para melhor organização e análise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "NoiJ5YPRs_3g",
    "outputId": "3d29b3ef-bff9-4ece-90dd-ad2e94105dc7"
   },
   "outputs": [],
   "source": [
    "docs_path = \"PATH TO YOUR DOCS\"\n",
    "documents = extract_docx_contents(docs_path)\n",
    "df = pd.DataFrame(documents)\n",
    "df.columns = [\"Title\", \"Content\"]\n",
    "df[\"Content\"] = df[\"Content\"].str.replace(r\"\\t|\\n\", \"\", regex=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Asp__XR0v3j5"
   },
   "outputs": [],
   "source": [
    "def embed_fn(title, text):\n",
    "  \"\"\"\n",
    "    Generates an embedding for the provided title and text using a specific model.\n",
    "\n",
    "    This function utilizes Google Generative AI to generate an embedding for the provided title and text.\n",
    "    The resulting embedding is useful for document retrieval tasks, where documents can be compared\n",
    "    based on the similarity of their embeddings.\n",
    "\n",
    "    Args:\n",
    "        title (str): The title of the document.\n",
    "        text (str): The text of the document.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The embedding generated for the provided title and text.\n",
    "\n",
    "    Example:\n",
    "        >>> title = \"Example Title\"\n",
    "        >>> text = \"This is an example text.\"\n",
    "        >>> embedding = embed_fn(title, text)\n",
    "    \"\"\"\n",
    "\n",
    "  return genai.embed_content(model=model,\n",
    "                                 content=text,\n",
    "                                 title=title,\n",
    "                                 task_type=\"RETRIEVAL_DOCUMENT\")[\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AXFFP2zN_to"
   },
   "source": [
    "A linha abaixo utiliza a função `embed_fn` para gerar embeddings para o título e o conteúdo de cada documento presente no DataFrame `df`. Esses embeddings são armazenados na coluna \"Embeddings\" do DataFrame para posterior análise ou processamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "7Wnn6e1swQfg",
    "outputId": "ca847372-66a2-4941-9767-bfc61af47436"
   },
   "outputs": [],
   "source": [
    "df[\"Embeddings\"] = df.apply(lambda row: embed_fn(row[\"Title\"], row[\"Content\"]), axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5baCqjr0O0KM"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Buscas\n",
    "\n",
    "Para simplificar a busca dentro dos documentos, o argumento `query` nas funções abaixo pode ser um trecho específico presente nos arquivos, uma parte do documento ou até mesmo algo relacionado ao tópico que você está procurando.\n",
    "\n",
    "Essas funções foram projetadas para facilitar a localização de documentos relevantes com base em consultas específicas. Basta fornecer um termo ou frase relevante e as funções encontrarão os documentos mais adequados dentro da base de dados.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LxyfqY8LxxHx"
   },
   "outputs": [],
   "source": [
    "def generate_and_search_query(query, database, model):\n",
    "    \"\"\"\n",
    "    Generates and searches a query within a given database using a specified model.\n",
    "\n",
    "    This function generates an embedding for the provided query using the specified model.\n",
    "    It then compares this embedding with the embeddings of documents in the database\n",
    "    to find the most relevant document.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query to be searched within the database.\n",
    "        database (DataFrame): The DataFrame containing the database of documents.\n",
    "        model (str): The model to be used for generating embeddings.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the most relevant document to the query.\n",
    "\n",
    "    Example:\n",
    "        >>> query = \"Example query\"\n",
    "        >>> relevant_document = generate_and_search_query(query, df, \"models/embedding-001\")\n",
    "    \"\"\"\n",
    "    embedding_of_query = genai.embed_content(model=model,\n",
    "                                             content=query,\n",
    "                                             task_type=\"RETRIEVAL_QUERY\")[\"embedding\"]\n",
    "\n",
    "    dot_products = np.dot(np.stack(database[\"Embeddings\"]), embedding_of_query)\n",
    "    index = np.argmax(dot_products)\n",
    "\n",
    "    return database.iloc[index][\"Content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "X9FmOZ9N61iI"
   },
   "outputs": [],
   "source": [
    "def generate_and_search_top_3_docs(query, database, model, similarity_threshold=0.65):\n",
    "    \"\"\"\n",
    "    Generates and searches a query within a given database using a specified model, returning the top 3 most relevant documents with a similarity score equal to or greater than the specified threshold.\n",
    "\n",
    "    This function generates an embedding for the provided query using the specified model.\n",
    "    It then compares this embedding with the embeddings of documents in the database\n",
    "    to find the top 3 most relevant documents with a similarity score equal to or greater than the specified threshold.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query to be searched within the database.\n",
    "        database (DataFrame): The DataFrame containing the database of documents.\n",
    "        model (str): The model to be used for generating embeddings.\n",
    "        similarity_threshold (float, optional): The minimum similarity score required for a document to be considered relevant. Defaults to 0.65.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples containing the content of the top 3 most relevant documents to the query, along with their similarity scores.\n",
    "\n",
    "    Example:\n",
    "        >>> query = \"Example query\"\n",
    "        >>> top_3_documents = generate_and_search_top_3_docs(query, df, \"models/embedding-001\")\n",
    "    \"\"\"\n",
    "    embedding_of_query = genai.embed_content(model=model,\n",
    "                                             content=query,\n",
    "                                             task_type=\"RETRIEVAL_QUERY\")[\"embedding\"]\n",
    "\n",
    "    embeddings_of_database = np.stack(database[\"Embeddings\"])\n",
    "    similarities = cosine_similarity(embeddings_of_database, np.array(embedding_of_query).reshape(1, -1)).flatten()\n",
    "    relevant_indices = np.where(similarities >= similarity_threshold)[0]\n",
    "    sorted_indices = np.argsort(similarities[relevant_indices])[::-1][:3]\n",
    "\n",
    "    top_relevant_documents = [(database.iloc[relevant_indices[index]][\"Content\"], similarities[relevant_indices[index]]) for index in sorted_indices]\n",
    "\n",
    "    return top_relevant_documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "il9Ps7MMPXiz"
   },
   "source": [
    "## Integração com Gemini AI\n",
    "\n",
    "Para integrar com o Gemini AI, configure as definições de geração da seguinte forma para que seja retornado apenas uma resposta da API. Exemplos de prompts para português e inglês foram adicionados. Para maior precisão ao documento use \"Não adicione nenhuma informação extra ao seguinte trecho.\". Dessa forma a genai não vai adicionar informações extras ou suposições na resposta. A API do Gemini só aceita um documento sendo enviado por vez como trecho, então selecione qual documento quer usar quando for enviar para a API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "bDHQlEuV4Jay",
    "outputId": "07b8bce0-ae77-49b0-8501-1d6f8af7a5e2"
   },
   "outputs": [],
   "source": [
    "query = \"WRITE YOUR QUERY HERE\"\n",
    "\n",
    "top_document = generate_and_search_query(query, df, model)\n",
    "\n",
    "\n",
    "generation_config = {\n",
    "  \"temperature\": 0,\n",
    "  \"candidate_count\": 1\n",
    "}\n",
    "\n",
    "prompt_br = f\"Me faça um resumo do texto. Não adicione nenhuma informação extra ao seguinte trecho. Trecho: {top_document}\"\n",
    "prompt_eng = f\"Ask your question here. Do not add any extra information to the following passage. Passage: {top_document}\"\n",
    "\n",
    "model_2 = genai.GenerativeModel(\"gemini-1.0-pro\",\n",
    "                                generation_config=generation_config)\n",
    "response = model_2.generate_content(prompt_br)\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
